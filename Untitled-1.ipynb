{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k='transformer.wte.weight' with shape torch.Size([50257, 768])\n",
      "k='transformer.wpe.weight' with shape torch.Size([1024, 768])\n",
      "k='transformer.h.0.ln_1.weight' with shape torch.Size([768])\n",
      "k='transformer.h.0.ln_1.bias' with shape torch.Size([768])\n",
      "k='transformer.h.0.attn.c_attn.weight' with shape torch.Size([768, 2304])\n",
      "k='transformer.h.0.attn.c_attn.bias' with shape torch.Size([2304])\n",
      "k='transformer.h.0.attn.c_proj.weight' with shape torch.Size([768, 768])\n",
      "k='transformer.h.0.attn.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.0.ln_2.weight' with shape torch.Size([768])\n",
      "k='transformer.h.0.ln_2.bias' with shape torch.Size([768])\n",
      "k='transformer.h.0.mlp.c_fc.weight' with shape torch.Size([768, 3072])\n",
      "k='transformer.h.0.mlp.c_fc.bias' with shape torch.Size([3072])\n",
      "k='transformer.h.0.mlp.c_proj.weight' with shape torch.Size([3072, 768])\n",
      "k='transformer.h.0.mlp.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.1.ln_1.weight' with shape torch.Size([768])\n",
      "k='transformer.h.1.ln_1.bias' with shape torch.Size([768])\n",
      "k='transformer.h.1.attn.c_attn.weight' with shape torch.Size([768, 2304])\n",
      "k='transformer.h.1.attn.c_attn.bias' with shape torch.Size([2304])\n",
      "k='transformer.h.1.attn.c_proj.weight' with shape torch.Size([768, 768])\n",
      "k='transformer.h.1.attn.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.1.ln_2.weight' with shape torch.Size([768])\n",
      "k='transformer.h.1.ln_2.bias' with shape torch.Size([768])\n",
      "k='transformer.h.1.mlp.c_fc.weight' with shape torch.Size([768, 3072])\n",
      "k='transformer.h.1.mlp.c_fc.bias' with shape torch.Size([3072])\n",
      "k='transformer.h.1.mlp.c_proj.weight' with shape torch.Size([3072, 768])\n",
      "k='transformer.h.1.mlp.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.2.ln_1.weight' with shape torch.Size([768])\n",
      "k='transformer.h.2.ln_1.bias' with shape torch.Size([768])\n",
      "k='transformer.h.2.attn.c_attn.weight' with shape torch.Size([768, 2304])\n",
      "k='transformer.h.2.attn.c_attn.bias' with shape torch.Size([2304])\n",
      "k='transformer.h.2.attn.c_proj.weight' with shape torch.Size([768, 768])\n",
      "k='transformer.h.2.attn.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.2.ln_2.weight' with shape torch.Size([768])\n",
      "k='transformer.h.2.ln_2.bias' with shape torch.Size([768])\n",
      "k='transformer.h.2.mlp.c_fc.weight' with shape torch.Size([768, 3072])\n",
      "k='transformer.h.2.mlp.c_fc.bias' with shape torch.Size([3072])\n",
      "k='transformer.h.2.mlp.c_proj.weight' with shape torch.Size([3072, 768])\n",
      "k='transformer.h.2.mlp.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.3.ln_1.weight' with shape torch.Size([768])\n",
      "k='transformer.h.3.ln_1.bias' with shape torch.Size([768])\n",
      "k='transformer.h.3.attn.c_attn.weight' with shape torch.Size([768, 2304])\n",
      "k='transformer.h.3.attn.c_attn.bias' with shape torch.Size([2304])\n",
      "k='transformer.h.3.attn.c_proj.weight' with shape torch.Size([768, 768])\n",
      "k='transformer.h.3.attn.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.3.ln_2.weight' with shape torch.Size([768])\n",
      "k='transformer.h.3.ln_2.bias' with shape torch.Size([768])\n",
      "k='transformer.h.3.mlp.c_fc.weight' with shape torch.Size([768, 3072])\n",
      "k='transformer.h.3.mlp.c_fc.bias' with shape torch.Size([3072])\n",
      "k='transformer.h.3.mlp.c_proj.weight' with shape torch.Size([3072, 768])\n",
      "k='transformer.h.3.mlp.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.4.ln_1.weight' with shape torch.Size([768])\n",
      "k='transformer.h.4.ln_1.bias' with shape torch.Size([768])\n",
      "k='transformer.h.4.attn.c_attn.weight' with shape torch.Size([768, 2304])\n",
      "k='transformer.h.4.attn.c_attn.bias' with shape torch.Size([2304])\n",
      "k='transformer.h.4.attn.c_proj.weight' with shape torch.Size([768, 768])\n",
      "k='transformer.h.4.attn.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.4.ln_2.weight' with shape torch.Size([768])\n",
      "k='transformer.h.4.ln_2.bias' with shape torch.Size([768])\n",
      "k='transformer.h.4.mlp.c_fc.weight' with shape torch.Size([768, 3072])\n",
      "k='transformer.h.4.mlp.c_fc.bias' with shape torch.Size([3072])\n",
      "k='transformer.h.4.mlp.c_proj.weight' with shape torch.Size([3072, 768])\n",
      "k='transformer.h.4.mlp.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.5.ln_1.weight' with shape torch.Size([768])\n",
      "k='transformer.h.5.ln_1.bias' with shape torch.Size([768])\n",
      "k='transformer.h.5.attn.c_attn.weight' with shape torch.Size([768, 2304])\n",
      "k='transformer.h.5.attn.c_attn.bias' with shape torch.Size([2304])\n",
      "k='transformer.h.5.attn.c_proj.weight' with shape torch.Size([768, 768])\n",
      "k='transformer.h.5.attn.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.5.ln_2.weight' with shape torch.Size([768])\n",
      "k='transformer.h.5.ln_2.bias' with shape torch.Size([768])\n",
      "k='transformer.h.5.mlp.c_fc.weight' with shape torch.Size([768, 3072])\n",
      "k='transformer.h.5.mlp.c_fc.bias' with shape torch.Size([3072])\n",
      "k='transformer.h.5.mlp.c_proj.weight' with shape torch.Size([3072, 768])\n",
      "k='transformer.h.5.mlp.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.6.ln_1.weight' with shape torch.Size([768])\n",
      "k='transformer.h.6.ln_1.bias' with shape torch.Size([768])\n",
      "k='transformer.h.6.attn.c_attn.weight' with shape torch.Size([768, 2304])\n",
      "k='transformer.h.6.attn.c_attn.bias' with shape torch.Size([2304])\n",
      "k='transformer.h.6.attn.c_proj.weight' with shape torch.Size([768, 768])\n",
      "k='transformer.h.6.attn.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.6.ln_2.weight' with shape torch.Size([768])\n",
      "k='transformer.h.6.ln_2.bias' with shape torch.Size([768])\n",
      "k='transformer.h.6.mlp.c_fc.weight' with shape torch.Size([768, 3072])\n",
      "k='transformer.h.6.mlp.c_fc.bias' with shape torch.Size([3072])\n",
      "k='transformer.h.6.mlp.c_proj.weight' with shape torch.Size([3072, 768])\n",
      "k='transformer.h.6.mlp.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.7.ln_1.weight' with shape torch.Size([768])\n",
      "k='transformer.h.7.ln_1.bias' with shape torch.Size([768])\n",
      "k='transformer.h.7.attn.c_attn.weight' with shape torch.Size([768, 2304])\n",
      "k='transformer.h.7.attn.c_attn.bias' with shape torch.Size([2304])\n",
      "k='transformer.h.7.attn.c_proj.weight' with shape torch.Size([768, 768])\n",
      "k='transformer.h.7.attn.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.7.ln_2.weight' with shape torch.Size([768])\n",
      "k='transformer.h.7.ln_2.bias' with shape torch.Size([768])\n",
      "k='transformer.h.7.mlp.c_fc.weight' with shape torch.Size([768, 3072])\n",
      "k='transformer.h.7.mlp.c_fc.bias' with shape torch.Size([3072])\n",
      "k='transformer.h.7.mlp.c_proj.weight' with shape torch.Size([3072, 768])\n",
      "k='transformer.h.7.mlp.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.8.ln_1.weight' with shape torch.Size([768])\n",
      "k='transformer.h.8.ln_1.bias' with shape torch.Size([768])\n",
      "k='transformer.h.8.attn.c_attn.weight' with shape torch.Size([768, 2304])\n",
      "k='transformer.h.8.attn.c_attn.bias' with shape torch.Size([2304])\n",
      "k='transformer.h.8.attn.c_proj.weight' with shape torch.Size([768, 768])\n",
      "k='transformer.h.8.attn.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.8.ln_2.weight' with shape torch.Size([768])\n",
      "k='transformer.h.8.ln_2.bias' with shape torch.Size([768])\n",
      "k='transformer.h.8.mlp.c_fc.weight' with shape torch.Size([768, 3072])\n",
      "k='transformer.h.8.mlp.c_fc.bias' with shape torch.Size([3072])\n",
      "k='transformer.h.8.mlp.c_proj.weight' with shape torch.Size([3072, 768])\n",
      "k='transformer.h.8.mlp.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.9.ln_1.weight' with shape torch.Size([768])\n",
      "k='transformer.h.9.ln_1.bias' with shape torch.Size([768])\n",
      "k='transformer.h.9.attn.c_attn.weight' with shape torch.Size([768, 2304])\n",
      "k='transformer.h.9.attn.c_attn.bias' with shape torch.Size([2304])\n",
      "k='transformer.h.9.attn.c_proj.weight' with shape torch.Size([768, 768])\n",
      "k='transformer.h.9.attn.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.9.ln_2.weight' with shape torch.Size([768])\n",
      "k='transformer.h.9.ln_2.bias' with shape torch.Size([768])\n",
      "k='transformer.h.9.mlp.c_fc.weight' with shape torch.Size([768, 3072])\n",
      "k='transformer.h.9.mlp.c_fc.bias' with shape torch.Size([3072])\n",
      "k='transformer.h.9.mlp.c_proj.weight' with shape torch.Size([3072, 768])\n",
      "k='transformer.h.9.mlp.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.10.ln_1.weight' with shape torch.Size([768])\n",
      "k='transformer.h.10.ln_1.bias' with shape torch.Size([768])\n",
      "k='transformer.h.10.attn.c_attn.weight' with shape torch.Size([768, 2304])\n",
      "k='transformer.h.10.attn.c_attn.bias' with shape torch.Size([2304])\n",
      "k='transformer.h.10.attn.c_proj.weight' with shape torch.Size([768, 768])\n",
      "k='transformer.h.10.attn.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.10.ln_2.weight' with shape torch.Size([768])\n",
      "k='transformer.h.10.ln_2.bias' with shape torch.Size([768])\n",
      "k='transformer.h.10.mlp.c_fc.weight' with shape torch.Size([768, 3072])\n",
      "k='transformer.h.10.mlp.c_fc.bias' with shape torch.Size([3072])\n",
      "k='transformer.h.10.mlp.c_proj.weight' with shape torch.Size([3072, 768])\n",
      "k='transformer.h.10.mlp.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.11.ln_1.weight' with shape torch.Size([768])\n",
      "k='transformer.h.11.ln_1.bias' with shape torch.Size([768])\n",
      "k='transformer.h.11.attn.c_attn.weight' with shape torch.Size([768, 2304])\n",
      "k='transformer.h.11.attn.c_attn.bias' with shape torch.Size([2304])\n",
      "k='transformer.h.11.attn.c_proj.weight' with shape torch.Size([768, 768])\n",
      "k='transformer.h.11.attn.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.h.11.ln_2.weight' with shape torch.Size([768])\n",
      "k='transformer.h.11.ln_2.bias' with shape torch.Size([768])\n",
      "k='transformer.h.11.mlp.c_fc.weight' with shape torch.Size([768, 3072])\n",
      "k='transformer.h.11.mlp.c_fc.bias' with shape torch.Size([3072])\n",
      "k='transformer.h.11.mlp.c_proj.weight' with shape torch.Size([3072, 768])\n",
      "k='transformer.h.11.mlp.c_proj.bias' with shape torch.Size([768])\n",
      "k='transformer.ln_f.weight' with shape torch.Size([768])\n",
      "k='transformer.ln_f.bias' with shape torch.Size([768])\n",
      "k='lm_head.weight' with shape torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "state = model.state_dict()\n",
    "for k, v in state.items():\n",
    "    print(f\"{k=} with shape {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/transformers/: cannot open `/root/.cache/huggingface/transformers/' (No such file or directory)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!file ~/.cache/huggingface/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "promt = \"Hello, I'm a language model,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE  Untitled-1.ipynb  inpu.txt  main.py  model.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "text = text[:1000]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 314, 1101, 257, 3303, 2746, 11]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(promt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.tensor([enc.encode(promt)])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'to_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m enc\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'to_list'"
     ]
    }
   ],
   "source": [
    "enc.decode(t[0].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 314, 1101, 257, 3303, 2746, 11]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(t[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((3,4,5))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0631,  0.3057, -0.7746,  0.0349,  0.3211],\n",
       "        [-0.9224,  1.8113,  0.1606,  0.3672,  0.1754],\n",
       "        [-0.8278,  1.3347,  0.4835, -0.1976,  1.2683]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
